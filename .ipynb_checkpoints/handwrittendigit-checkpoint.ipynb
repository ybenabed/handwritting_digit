{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1).Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2).Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_mnist_data(path=\"train-images-idx3-ubyte.gz\", num_images=60000):\n",
    "    dataset_file = gzip.open(path,\"r\")\n",
    "    first_pixel_at = 16\n",
    "    image_size = 28\n",
    "    dataset_file.read(first_pixel_at)\n",
    "    buf = dataset_file.read(image_size * image_size * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float64)\n",
    "    data = data.reshape(num_images, image_size, image_size, 1)\n",
    "    images = np.asarray(data).squeeze()\n",
    "    dataset_file.close()\n",
    "    return images\n",
    "\n",
    "def read_mnist_label(path=\"train-labels-idx1-ubyte.gz\", num_labels=60000):\n",
    "    dataset_file = gzip.open(path,\"r\")\n",
    "    first_label_at = 8\n",
    "    dataset_file.read(first_label_at)\n",
    "    buf = dataset_file.read(num_labels)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    data = data.reshape(num_labels, 1)\n",
    "    return data\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "index = 5467\n",
    "labels = read_mnist_label()\n",
    "print(\"Number:\", labels[index])\n",
    "images = read_mnist_data()\n",
    "show_image(images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(images):\n",
    "    images = images.reshape(images.shape[0], images.shape[1]*images.shape[2])\n",
    "    return images.T / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels encoding with OneHot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(Y):\n",
    "    size = (Y.max()+1, Y.size)\n",
    "    onehot = np.zeros(size)\n",
    "    onehot[Y, np.arange(Y.size)] = 1\n",
    "    return onehot\n",
    "\n",
    "onehot = onehot_encoding(labels.reshape(labels.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x_train = flatten_images(read_mnist_data())\n",
    "    y_train = read_mnist_label()\n",
    "    y_train = onehot_encoding(y_train.reshape(y_train.shape[0]))\n",
    "    x_test = flatten_images(read_mnist_data(path=\"t10k-images-idx3-ubyte.gz\", num_images=10000))\n",
    "    y_test = read_mnist_label(path=\"t10k-labels-idx1-ubyte.gz\", num_labels=10000)\n",
    "    y_test = onehot_encoding(y_test.reshape(y_test.shape[0]))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3).Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    # We are using HE initialization\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        #parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) *np.sqrt(2/(layer_dims[l-1]+layer_dims[l]))\n",
    "        #parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.0001\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        print(\"W\"+str(l)+\" is of shape:\", )\n",
    "    return parameters\n",
    "\n",
    "def initialize_adam_parameters(parameters):\n",
    "\tL = len(parameters) // 2\n",
    "\tv = {}\n",
    "\ts = {}\n",
    "\n",
    "\tfor l in range(L):\n",
    "\t\tv[\"W\"+str(l+1)] = np.zeros(( parameters[\"W\"+str(l+1)].shape[0], parameters[\"W\"+str(l+1)].shape[1]))\n",
    "\t\tv[\"b\"+str(l+1)] = np.zeros(( parameters[\"b\"+str(l+1)].shape[0], parameters[\"b\"+str(l+1)].shape[1]))\n",
    "\t\ts[\"W\"+str(l+1)] = np.zeros(( parameters[\"W\"+str(l+1)].shape[0], parameters[\"W\"+str(l+1)].shape[1]))\n",
    "\t\ts[\"b\"+str(l+1)] = np.zeros(( parameters[\"b\"+str(l+1)].shape[0], parameters[\"b\"+str(l+1)].shape[1]))\n",
    "\n",
    "\treturn v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation=\"relu\"):\n",
    "    \n",
    "    Z, linear_cache = np.dot(W, A_prev) + b, (A_prev, W, b)\n",
    "    \n",
    "    if activation==\"relu\":\n",
    "        A, activation_cache = np.maximum(0, Z), np.maximum(0, Z)\n",
    "        \n",
    "    if activation==\"softmax\":\n",
    "        expZ = np.exp(Z)\n",
    "        A, activation_cache = expZ / np.sum(expZ, axis=0), expZ / np.sum(expZ, axis=0)\n",
    "        \n",
    "    if activation==\"sigmoid\":\n",
    "        A, activation_cache = 1 / (1 + np.exp(-Z)), 1 / (1 + np.exp(-Z))\n",
    "        \n",
    "    cache = (linear_cache, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W, b = parameters[\"W\"+str(l)], parameters[\"b\"+str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    W, b = parameters[\"W\"+str(L)], parameters[\"b\"+str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.072958576622734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    J = np.mean( - np.sum( Y * np.log(AL + 1e-8), axis=0 ) )\n",
    "    return J\n",
    "\n",
    "Y_hat = np.array([ [.4, 0.2, 0.3], [0.3, 0.1, 0.2], [0.3, 0.7, .5] ])\n",
    "Y = np.array([ [1., 1., 0.], [0., 0., 0.], [0., 0., 1.] ])\n",
    "compute_cost(Y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "### a-Derivatives\n",
    "#### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(A, Y):\n",
    "    return A - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b-Backward Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.mean(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation=\"relu\"):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation==\"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    if activation==\"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    return linear_backward(dZ, linear_cache)\n",
    "\n",
    "def linear_activation_backward_softmax(AL, Y, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = softmax_backward(AL, Y)\n",
    "    return linear_backward(dZ + 1e-8, linear_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    derivatives = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    cache = caches[L-1]\n",
    "    (derivatives[\"dA\" + str(L-1)], derivatives[\"dW\" + str(L)], \n",
    "     derivatives[\"db\" + str(L)]) = linear_activation_backward_softmax(AL, Y, cache)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        cache = caches[l]\n",
    "        (derivatives[\"dA\" + str(l)], derivatives[\"dW\" + str(l+1)],\n",
    "         derivatives[\"db\" + str(l+1)]) = linear_activation_backward(derivatives[\"dA\" + str(l+1)], cache, \"sigmoid\")\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, derivatives, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 #Â Here we get the number of layers\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] -= learning_rate * derivatives[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)] -= learning_rate * derivatives[\"db\"+str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def update_parameters_with_adams(parameters, derivatives, v, s, t, learning_rate, beta1=0.9,\n",
    "                                 beta2=0.999, epsilon=1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    \n",
    "    s_corrected = {}\n",
    "    v_corrected = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        v[\"W\"+str(l+1)] = beta1 * v[\"W\"+str(l+1)] + (1 - beta1) * derivatives[\"dW\"+str(l+1)]\n",
    "        v[\"b\"+str(l+1)] = beta1 * v[\"b\"+str(l+1)] + (1 - beta1) * derivatives[\"db\"+str(l+1)]\n",
    "        s[\"W\"+str(l+1)] = beta2 * s[\"W\"+str(l+1)] + (1 -beta2) * (derivatives[\"dW\"+str(l+1)] ** 2)\n",
    "        s[\"b\"+str(l+1)] = beta2 * s[\"b\"+str(l+1)] + (1 -beta2) * (derivatives[\"db\"+str(l+1)] ** 2)\n",
    "        v_corrected[\"W\"+str(l+1)] = v[\"W\"+str(l+1)] / (1 - beta1 ** t)\n",
    "        v_corrected[\"b\"+str(l+1)] = v[\"b\"+str(l+1)] / (1 - beta1 ** t)\n",
    "        s_corrected[\"W\"+str(l+1)] = s[\"W\"+str(l+1)] / (1 - beta2 ** t)\n",
    "        s_corrected[\"b\"+str(l+1)] = s[\"b\"+str(l+1)] / (1 - beta2 ** t)\n",
    "        parameters[\"W\"+str(l+1)] -= learning_rate * v_corrected[\"W\"+str(l+1)] / (np.sqrt(s_corrected[\"W\"+str(l+1)]) + epsilon)\n",
    "        parameters[\"b\"+str(l+1)] -= learning_rate * v_corrected[\"b\"+str(l+1)] / (np.sqrt(s_corrected[\"b\"+str(l+1)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the L-layers NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, learning_rate=0.01, layer_dims=[28*28,40,10], num_iteration=2500, optim=\"none\"):\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    if optim==\"adam\":\n",
    "        v, s = initialize_adam_parameters(parameters)\n",
    "    for i in range(num_iteration):\n",
    "        #Forward Prop\n",
    "        Z3, caches = forward_propagation(X, parameters)\n",
    "        #Compute the cost\n",
    "        cost = compute_cost(Z3, Y)\n",
    "        #BackProp\n",
    "        derivatives = backward_propagation(Z3, Y, caches)\n",
    "        #Update parameteres\n",
    "        #parameters = update_parameters(parameters, derivatives, learning_rate)\n",
    "        if optim==\"none\":\n",
    "            parameters = update_parameters(parameters, derivatives, learning_rate)\n",
    "        elif optim==\"adam\":\n",
    "            parameters, v, s = update_parameters_with_adams(parameters,derivatives, v, s, i+1, learning_rate)\n",
    "        #Print the cost\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"Cost after\",i+1,\"iteration:\",str(cost),\"learning Rate:\",learning_rate)\n",
    "        if cost <= 0.02: break\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    Z3, cache = forward_propagation(X, parameters)\n",
    "    Y_hat = np.argmax(Z3, axis=0)\n",
    "    Y = np.argmax(Y, axis=0)\n",
    "    accuracy = (Y_hat == Y).astype(int).mean()\n",
    "    return Y, accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 20 iteration: 0.7615331606766922 learning Rate: 0.01\n",
      "Cost after 40 iteration: 0.4055600416408738 learning Rate: 0.01\n",
      "Cost after 60 iteration: 0.3079745633099882 learning Rate: 0.01\n",
      "Cost after 80 iteration: 0.2615781834047755 learning Rate: 0.01\n",
      "Cost after 100 iteration: 0.2318390125476648 learning Rate: 0.01\n",
      "Cost after 120 iteration: 0.20971538775696158 learning Rate: 0.01\n",
      "Cost after 140 iteration: 0.19204098526079316 learning Rate: 0.01\n",
      "Cost after 160 iteration: 0.17724764560997852 learning Rate: 0.01\n",
      "Cost after 180 iteration: 0.16448437907565003 learning Rate: 0.01\n",
      "Cost after 200 iteration: 0.1532413676356924 learning Rate: 0.01\n",
      "Cost after 220 iteration: 0.14317342061937335 learning Rate: 0.01\n",
      "Cost after 240 iteration: 0.1340582477132877 learning Rate: 0.01\n",
      "Cost after 260 iteration: 0.1258067392968187 learning Rate: 0.01\n",
      "Cost after 280 iteration: 0.11833840232341829 learning Rate: 0.01\n",
      "Cost after 300 iteration: 0.11152653614498495 learning Rate: 0.01\n",
      "Cost after 320 iteration: 0.10528143338090805 learning Rate: 0.01\n",
      "Cost after 340 iteration: 0.09957536135082215 learning Rate: 0.01\n",
      "Cost after 360 iteration: 0.09434263788695314 learning Rate: 0.01\n",
      "Cost after 380 iteration: 0.08951602969446718 learning Rate: 0.01\n",
      "Cost after 400 iteration: 0.0850408917378252 learning Rate: 0.01\n",
      "Cost after 420 iteration: 0.08087885275831067 learning Rate: 0.01\n",
      "Cost after 440 iteration: 0.077000825717703 learning Rate: 0.01\n",
      "Cost after 460 iteration: 0.07337648717911097 learning Rate: 0.01\n",
      "Cost after 480 iteration: 0.0699800471265045 learning Rate: 0.01\n",
      "Cost after 500 iteration: 0.06679397630080156 learning Rate: 0.01\n",
      "Cost after 520 iteration: 0.06379638826349478 learning Rate: 0.01\n",
      "Cost after 540 iteration: 0.06096882666551952 learning Rate: 0.01\n",
      "Cost after 560 iteration: 0.05829846045876013 learning Rate: 0.01\n",
      "Cost after 580 iteration: 0.05577732448347877 learning Rate: 0.01\n",
      "Cost after 600 iteration: 0.05339681391748709 learning Rate: 0.01\n",
      "Cost after 620 iteration: 0.051145929106274926 learning Rate: 0.01\n",
      "Cost after 640 iteration: 0.0490116881381781 learning Rate: 0.01\n",
      "Cost after 660 iteration: 0.04699124434729112 learning Rate: 0.01\n",
      "Cost after 680 iteration: 0.04507528984972602 learning Rate: 0.01\n",
      "Cost after 700 iteration: 0.04325314468258895 learning Rate: 0.01\n",
      "Cost after 720 iteration: 0.041516117139475436 learning Rate: 0.01\n",
      "Cost after 740 iteration: 0.039856850664581056 learning Rate: 0.01\n",
      "Cost after 760 iteration: 0.03826897796401257 learning Rate: 0.01\n",
      "Cost after 780 iteration: 0.036746053134047336 learning Rate: 0.01\n",
      "Cost after 800 iteration: 0.035282869652652156 learning Rate: 0.01\n",
      "Cost after 820 iteration: 0.03388395345526603 learning Rate: 0.01\n",
      "Cost after 840 iteration: 0.03254812275015204 learning Rate: 0.01\n",
      "Cost after 860 iteration: 0.03127072827224736 learning Rate: 0.01\n",
      "Cost after 880 iteration: 0.03004808919065638 learning Rate: 0.01\n",
      "Cost after 900 iteration: 0.028877337081352178 learning Rate: 0.01\n",
      "Cost after 920 iteration: 0.027756427588757963 learning Rate: 0.01\n",
      "Cost after 940 iteration: 0.026683014382417027 learning Rate: 0.01\n",
      "Cost after 960 iteration: 0.025655765008266614 learning Rate: 0.01\n",
      "Cost after 980 iteration: 0.024677653035786423 learning Rate: 0.01\n",
      "Cost after 1000 iteration: 0.02374926566252125 learning Rate: 0.01\n",
      "Cost after 1020 iteration: 0.022865202650371435 learning Rate: 0.01\n",
      "Cost after 1040 iteration: 0.022020943124653993 learning Rate: 0.01\n",
      "Cost after 1060 iteration: 0.021212934366002332 learning Rate: 0.01\n",
      "Cost after 1080 iteration: 0.020437998542774484 learning Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(x_train, y_train, learning_rate=0.01, layer_dims=[28*28,40,10], optim=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 99.80333333333333 %\n",
      "Test accuracy= 96.6 %\n"
     ]
    }
   ],
   "source": [
    "Y, train_accuracy = predict(x_train, y_train, parameters)\n",
    "Y, test_accuracy = predict(x_test, y_test, parameters)\n",
    "print(\"Training accuracy=\",train_accuracy,\"%\")\n",
    "print(\"Test accuracy=\",test_accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
